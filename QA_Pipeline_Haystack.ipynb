{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S4HRKBOY/HaystackQAPipelineExample/blob/main/QA_Pipeline_Haystack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/*release"
      ],
      "metadata": {
        "id": "0dE6_iw1ZHbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAFYHn55yC3b"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Install the latest main of Haystack\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab,ocr,preprocessing,file-conversion,pdf,elasticsearch,inference,faiss]\n",
        "pip install farm-haystack[beir]\n",
        "\n",
        "apt install libgraphviz-dev\n",
        "pip install pygraphviz\n",
        "\n",
        "pip install datasets\n",
        "pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar9ygKsnd_n0"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.DEBUG)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTiHUbnj0ujE"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APFmMBXT2hMo"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbZFe-ag2k9R"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "time.sleep(30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
        "document_store = ElasticsearchDocumentStore(host = \"localhost\",\n",
        "                                            port = 9200,\n",
        "                                            embedding_dim = 768)"
      ],
      "metadata": {
        "id": "reG--Dd3R28t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "top_k_retriever = 2\n",
        "\n",
        "retriever = EmbeddingRetriever(\n",
        "    top_k=top_k_retriever,\n",
        "    document_store=document_store,\n",
        "    embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
        "    model_format=\"sentence_transformers\"\n",
        ")"
      ],
      "metadata": {
        "id": "uRqOCIagSDma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## EVALUATION\n",
        "'''\n",
        "from haystack.nodes import PreProcessor\n",
        "from haystack.utils import fetch_archive_from_http\n",
        "\n",
        "doc_dir = \"data/eval\"\n",
        "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/nq_dev_subset_v2.json.zip\"\n",
        "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
        "\n",
        "label_preprocessor = PreProcessor(\n",
        "    clean_empty_lines=False,\n",
        "    clean_whitespace=False,\n",
        "    split_by=\"word\",\n",
        "    split_length=300,\n",
        "    split_overlap=0,\n",
        "    split_respect_sentence_boundary=False,\n",
        ")\n",
        "\n",
        "document_store.add_eval_data(\n",
        "    filename=\"data/eval/nq_dev_subset_v2.json\",\n",
        "    doc_index=document_store.index,\n",
        "    label_index=document_store.label_index,\n",
        "    preprocessor=label_preprocessor,\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "o27micuCU-g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "document_store.update_embeddings(retriever=retriever)\n",
        "retriever_eval_results = retriever.eval(top_k=5, label_index=document_store.label_index, doc_index=document_store.index)\n",
        "# Retriever Recall is the proportion of questions for which the correct document containing the answer is\n",
        "# among the correct documents\n",
        "print(\"Retriever Recall:\", retriever_eval_results[\"recall\"])\n",
        "# Retriever Mean Avg Precision rewards retrievers that give relevant documents a higher rank\n",
        "print(\"Retriever Mean Avg Precision:\", retriever_eval_results[\"map\"])\n",
        "'''\n",
        "# END OF EVALUATION"
      ],
      "metadata": {
        "id": "XHk48QIAaAOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLcJbGcz45ZA"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "wikipedia_entries = 15\n",
        "\n",
        "def process_wikipedia_query(question):\n",
        "  article_names = wikipedia.search(question, results=10)\n",
        "  wikipedia_documents = []\n",
        "  for article_name in article_names:\n",
        "    try:\n",
        "      wikipedia_documents.append(wikipedia.page(article_name).content)\n",
        "    except:\n",
        "      continue\n",
        "  return wikipedia_documents\n",
        "\n",
        "def retrive_article_titles(question):\n",
        "  return wikipedia.search(question, results=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsH2jmO_75T5"
      },
      "outputs": [],
      "source": [
        "doc_dir = \"data/wikipedia\"\n",
        "\n",
        "if not os.path.exists(doc_dir):\n",
        "  os.makedirs(doc_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra2PQtA8ErNm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def clear_data_dir():\n",
        "  files = glob.glob('data/wikipedia/*.txt')\n",
        "  for f in files:\n",
        "      os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elNjKFjkFTkt"
      },
      "outputs": [],
      "source": [
        "def create_files_for_retriever(articles,titles):\n",
        "  files = []\n",
        "  for article,title in zip(articles,titles):\n",
        "    title = title.replace(\"/\",\"\")\n",
        "    f = open(doc_dir + \"/\" + title + \".txt\", \"w\")\n",
        "    f.write(str(article.encode('utf-8', 'replace')))\n",
        "    files.append(f)\n",
        "    f.close()\n",
        "  return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jttVqCWbI-Ms"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
        "\n",
        "'''\n",
        "rag_prompt = PromptTemplate(\n",
        "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
        "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
        "                             Your answer should be in your own words and be not longer than 50 words.\n",
        "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
        "    output_parser=AnswerParser(),\n",
        ")\n",
        "\n",
        "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=rag_prompt, output_variable=\"my_answer\")\n",
        "'''\n",
        "'''\n",
        "elaboration_prompt = PromptTemplate(\n",
        "        prompt=\"\"\"Elaborate on the answer to the following question given the related texts.\n",
        "                                 Provide additional details to the answer in your own words.\n",
        "                                 The final response should be between 100-200 words.\n",
        "                                 \\n\\n Related text: {join(documents)} \\n\\n Question:\n",
        "              {questions} \\n\\n Previous answer: {my_answer} \\n\\n New answer:\"\"\",\n",
        "        output_parser=AnswerParser(),\n",
        "    )\n",
        "elaboration_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=elaboration_prompt)\n",
        "'''\n",
        "\n",
        "rag_prompt = PromptTemplate(\n",
        "    prompt=\"\"\"Elaborate on the answer to the following question given the related texts.\n",
        "                              \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\"\n",
        ")\n",
        "\n",
        "rag_node = PromptNode(\n",
        "    model_name_or_path=\"google/flan-t5-large\",\n",
        "    max_length=75,\n",
        "    default_prompt_template=rag_prompt,\n",
        "    use_gpu=True,\n",
        "    output_variable=\"my_answer\"\n",
        ")\n",
        "\n",
        "'''\n",
        "elaboration_prompt = PromptTemplate(\n",
        "    prompt=\"\"\"Elaborate on the answer to the following question given the related texts.\n",
        "                Provide additional details to the answer in your own words.\n",
        "                The final response should be between 100-200 words.\n",
        "                \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Previous answer: {my_answer} \\n\\n New answer: \"\"\"\n",
        ")\n",
        "elaboration_node = PromptNode(model_name_or_path=\"vblagoje/bart_lfqa\", default_prompt_template=elaboration_prompt)\n",
        "'''\n",
        "\n",
        "'''\n",
        "  Models tried: vblagoje/bart_lfqa,\n",
        "                pszemraj/t5-base-askscience-lfqa\n",
        "                google/flan-t5-large\n",
        "                google/flan-ul2\n",
        "  Prompts tried:\n",
        "    Synthesize a comprehensive answer from the following text for the given question.\n",
        "                                Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
        "                                Your answer should be in your own words and be no longer than 50 words.\n",
        "                                \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\n",
        "  PromptNode can be adjusted with the use of different models and different Prompts!\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTz_azAIK_rp"
      },
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.nodes import TextConverter, PreProcessor\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "text_converter = TextConverter()\n",
        "\n",
        "indexing_pipeline.add_node(component=text_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
        "indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"TextConverter\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sPVVJ1JnC8i"
      },
      "outputs": [],
      "source": [
        "def get_files_to_index():\n",
        "  files_to_index = []\n",
        "  for f in os.listdir(doc_dir):\n",
        "    if(isinstance(f,TextIOWrapper)):\n",
        "      files_to_index.append(doc_dir + \"/\" + f)\n",
        "  return files_to_index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = PreProcessor(\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=True,\n",
        "    clean_empty_lines=True,\n",
        "    split_by=\"word\",\n",
        "    split_length=100,\n",
        "    split_overlap=10,\n",
        "    split_respect_sentence_boundary=True,\n",
        ")\n",
        "\n",
        "def process_documents():\n",
        "  docs = []\n",
        "  unprocessed_documents = document_store.get_all_documents()\n",
        "  document_store.delete_documents()\n",
        "\n",
        "  processed_documents = preprocessor.process(unprocessed_documents)\n",
        "  embeds = retriever.embed_documents(processed_documents)\n",
        "  for i, doc in enumerate(processed_documents):\n",
        "    doc.embedding = embeds[i]\n",
        "  document_store.write_documents(processed_documents)\n",
        "  docs.clear()\n"
      ],
      "metadata": {
        "id": "wI-tToATTMoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRNHd0ikiQRO"
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline()\n",
        "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
        "pipe.add_node(component=rag_node, name=\"rag_node\", inputs=[\"retriever\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_pipeline.draw()"
      ],
      "metadata": {
        "id": "O4A8v15gjrM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.draw()"
      ],
      "metadata": {
        "id": "2ZuE02LdlS6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHq9LuXjeBAK"
      },
      "outputs": [],
      "source": [
        "from haystack.utils import fetch_archive_from_http, convert_files_to_docs, clean_wiki_text\n",
        "\n",
        "def process_question_and_create_answer(question):\n",
        "  clear_data_dir()\n",
        "  document_store.delete_documents()\n",
        "  wikipedia_documents = process_wikipedia_query(question)\n",
        "  wikipedia_titles = retrive_article_titles(question)\n",
        "  create_files_for_retriever(wikipedia_documents,wikipedia_titles)\n",
        "  files_to_index = [doc_dir + \"/\" + f  for f in os.listdir(doc_dir)]\n",
        "  # 1st Element needs to be removed!\n",
        "  files_to_index.pop(0)\n",
        "  if \"data/wikipedia/.ipynb_checkpoints\" in files_to_index:\n",
        "    files_to_index.remove(\"data/wikipedia/.ipynb_checkpoints\")\n",
        "  indexing_pipeline.run(file_paths=files_to_index)\n",
        "  process_documents()\n",
        "  output = pipe.run(query=question)\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8MCP041h5FX"
      },
      "outputs": [],
      "source": [
        "question = \"Why is the sky blue?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who was elected president of the USA in 2020?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "UrBHdR52DUKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"2 times 2 plus 3 equals what number?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "afd4El88E7Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is binary search?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "TYG9QKKPDsDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How does binary search work?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "zUTqne7wFcLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the answer to life?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "fSxV3wrREHRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who was Michael Jackson?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "kls0dm8vE7Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What does NaCl stands for?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "leH7oXA1FI2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Star Wars?\"\n",
        "result = process_question_and_create_answer(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "IB4w0OY6MCkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPoKOI1mMaYVnBeQlH5ejJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}